{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMznhlKPcaHmBKh+hLA4GTr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pewikr/hopff/blob/main/Forward_Hopfield.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GupiijIbyVQ7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def S(x):\n",
        "    # Define your activation function here\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def dS(x):\n",
        "    # Define the derivative of your activation function here\n",
        "    return S(x) * (1 - S(x))\n",
        "\n",
        "def normalize(input):\n",
        "    # Normalize the input to the range [0, 1]\n",
        "    return (input - np.min(input)) / (np.max(input) - np.min(input))\n",
        "\n",
        "class SimpleLayer:\n",
        "    def __init__(self, n_neurons, n_inputs, lambda_reg):\n",
        "        self.weights = np.random.rand(n_neurons, n_inputs)\n",
        "        self.bias = np.random.rand(n_neurons)\n",
        "        self.lambda_reg = lambda_reg  # regularization parameter\n",
        "\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        self.output = S(np.dot(self.weights, input) + self.bias)\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, output_error, learning_rate):\n",
        "        input_error = np.dot(output_error, self.weights)\n",
        "        weights_error = np.dot(output_error * dS(self.output), self.input.T) + self.lambda_reg * self.weights\n",
        "        self.weights -= learning_rate * weights_error\n",
        "        self.bias -= learning_rate * output_error\n",
        "        return input_error\n",
        "\n",
        "    def get_regularization_loss(self):\n",
        "        # L2 regularization\n",
        "        return 0.5 * self.lambda_reg * np.sum(np.square(self.weights))\n",
        "\n",
        "class AssociativeLayer:\n",
        "    def __init__(self, n_neurons, n_inputs, lambda_reg):\n",
        "        self.weights = np.random.rand(n_neurons, n_inputs)\n",
        "        self.weights_ll = np.random.rand(n_neurons, n_neurons)\n",
        "        self.bias = np.random.rand(n_neurons)\n",
        "        self.lambda_reg = lambda_reg  # regularization parameter\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Compute ACT_temp\n",
        "        self.input = input\n",
        "        self.ACT_temp = S(np.dot(self.weights, input))\n",
        "\n",
        "        # Compute the activation of the layer neurons\n",
        "        self.output = S(np.dot(self.weights_ll, self.ACT_temp) + np.dot(self.weights, input) + self.bias)\n",
        "\n",
        "        return self.output\n",
        "\n",
        "    def backward(self, output_error, learning_rate):\n",
        "        # Compute the derivative of the output\n",
        "        d_output = output_error * dS(self.output)\n",
        "\n",
        "        # Compute the error for weights_ll\n",
        "        weights_ll_error = np.dot(d_output, self.ACT_temp.T) + self.lambda_reg * self.weights_ll\n",
        "\n",
        "        # Compute the error for ACT_temp\n",
        "        ACT_temp_error = np.dot(d_output.T, self.weights_ll)\n",
        "\n",
        "        # Compute the error for weights\n",
        "        weights_error = np.dot(ACT_temp_error.T * dS(self.ACT_temp), self.input.T) + np.dot(d_output, self.input.T) + self.lambda_reg * self.weights\n",
        "\n",
        "        # Update the weights and biases\n",
        "        self.weights -= learning_rate * weights_error\n",
        "        self.weights_ll -= learning_rate * weights_ll_error\n",
        "        self.bias -= learning_rate * np.sum(d_output, axis=1, keepdims=True)\n",
        "\n",
        "        # Compute the error for the input\n",
        "        input_error = np.dot(ACT_temp_error.T * dS(self.ACT_temp), self.weights)\n",
        "\n",
        "        return input_error\n",
        "\n",
        "    def get_regularization_loss(self):\n",
        "        # L2 regularization\n",
        "        return 0.5 * self.lambda_reg * (np.sum(np.square(self.weights)) + np.sum(np.square(self.weights_ll)))\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, n_i, n_l, n_o, lambda_reg):\n",
        "        self.associative_layer = AssociativeLayer(n_l, n_i, lambda_reg)\n",
        "        self.output_layer = SimpleLayer(n_o, n_l, lambda_reg)\n",
        "\n",
        "    def forward(self, input):\n",
        "        # Normalize the input\n",
        "        input = normalize(input)\n",
        "\n",
        "        # Compute the activation of the associative layer\n",
        "        associative_activation = self.associative_layer.forward(input)\n",
        "\n",
        "        # Compute the output\n",
        "        output = self.output_layer.forward(associative_activation)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def compute_loss(self, output, target):\n",
        "        # Compute the mean squared error loss\n",
        "        mse_loss = np.mean((output - target) ** 2)\n",
        "\n",
        "        # Add the regularization loss\n",
        "        regularization_loss = self.get_total_regularization_loss()\n",
        "\n",
        "        # Total loss is the sum of the MSE loss and the regularization loss\n",
        "        total_loss = mse_loss + regularization_loss\n",
        "\n",
        "        return total_loss\n",
        "\n",
        "    def backward(self, output, target, learning_rate):\n",
        "        # Compute the error\n",
        "        output_error = 2 * (output - target) / output.size\n",
        "\n",
        "        # Backpropagate the error\n",
        "        associative_error = self.output_layer.backward(output_error, learning_rate)\n",
        "        self.associative_layer.backward(associative_error, learning_rate)\n",
        "\n",
        "    def get_total_regularization_loss(self):\n",
        "        # Total regularization loss is the sum of the regularization losses of all layers\n",
        "        return self.associative_layer.get_regularization_loss() + self.output_layer.get_regularization_loss()\n"
      ]
    }
  ]
}